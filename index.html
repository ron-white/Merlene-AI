<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MERLENE AI Assistant</title>
    <link rel="stylesheet" href="style.css">
    <script src="app.js" defer></script>
</head>

<body>
    <div class="mic-btn-container">
        <div class="response-window" id="responseWindow">
            <div class="response-history" id="responseHistory"></div>
        </div>
        <button class="mic-btn" id="micBtn" title="Activate Microphone">
            <span id="micIcon">ðŸŽ¤</span>
        </button>
        <div class="speech-text" id="speechText"></div>
    </div>
    <script>
        // Microphone button logic
        const micBtn = document.getElementById('micBtn');
        const speechText = document.getElementById('speechText');
        const micIcon = document.getElementById('micIcon');
        let listening = false;

        // Speech recognition setup
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = true;

        recognition.onstart = function () {
            listening = true;
            micBtn.classList.add('active');
            micIcon.textContent = 'ðŸ”´';
            speechText.textContent = 'Listening...';
        };
        recognition.onend = function () {
            listening = false;
            micBtn.classList.remove('active');
            micIcon.textContent = 'ðŸŽ¤';
        };
        recognition.onresult = function (event) {
            let transcript = '';
            for (let i = event.resultIndex; i < event.results.length; ++i) {
                transcript += event.results[i][0].transcript;
            }
            speechText.textContent = transcript;
            if (event.results[event.results.length - 1].isFinal) {
                window.speechSynthesis.cancel();
                if (typeof takeCommand === 'function') {
                    takeCommand(transcript.toLowerCase());
                }
            }
        };

        micBtn.onclick = function () {
            if (!listening) {
                recognition.start();
            } else {
                recognition.stop();
            }
        };

        // Typing effect for AI response
        function typeResponse(text, callback) {
            const responseWindow = document.getElementById('responseWindow');
            let i = 0;
            responseWindow.innerHTML = '';
            function type() {
                if (i < text.length) {
                    responseWindow.innerHTML += text.charAt(i);
                    i++;
                    setTimeout(type, 18);
                } else if (callback) {
                    callback();
                }
            }
            type();
        }

        // Save response to history
        function addToHistory(text) {
            const historyDiv = document.getElementById('responseHistory');
            const entry = document.createElement('div');
            entry.className = 'history-entry';
            entry.textContent = text;
            historyDiv.appendChild(entry);
            historyDiv.scrollTop = historyDiv.scrollHeight;
        }

        // Override speak to use typing effect and history
        window.speak = function(text) {
            typeResponse(text, function() {
                addToHistory(text);
            });
            const utter = new SpeechSynthesisUtterance(text);
            utter.rate = 1;
            utter.volume = 1;
            utter.pitch = 1;
            window.speechSynthesis.speak(utter);
        };
    </script>
</body>

</html>
